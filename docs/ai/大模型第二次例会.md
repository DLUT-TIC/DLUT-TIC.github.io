---
title: 大模型第二次例会
author: saber
date: 2024/9/9
---

>[!warning]
>文档内容可能存在些许错误

## 前置条件

###### 环境（推荐新建一个环境）
- 一个虚拟环境
```shell
conda create -n yourname
```

- **jupyter lab** （如果有装anaconda的话已经安装好了）
- 将新的虚拟环境加入notebook中
```shell
# 在base环境下
conda activate
conda install nb_conda_kernels

# 在要打开的环境
conda activate your_env_name
conda install ipykernel
```
###### 依赖项
```python
pip install --upgrade openai
pip list  # 查看有没有成功安装
```
###### 注册moonshot API
~~token免费送！~~
[Moonshot AI - 开放平台](https://platform.moonshot.cn/console/account)
## 大概纲要

- Transformer
	- Seq2Seq
	- self-attention
	- Encoder
	- Decoder
		- softmax
	- Encoder-decoder
- API参数
	- OpenAI为例

## Transformer
^48eae8

![Pasted image 20240712122336](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240712122336.png)

#### Seq2Seq
**Seq2Seq**（Sequence to Sequence，序列到序列模型）  
包括**编码器(Encoder)**，**解码器(Decoder)** 两部分。  
Seq2Seq 输出不定长度的序列  

>[!什么可以是不定长度的序列]
>文字
>语音(音频)
>	- 语音识别，语音翻译，聊天
>图片
>	- 拆解图像，RGB三维向量

#### self-attention

![Pasted image 20240629101948|500](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240629101948.png)
整个序列(向量组)输入，综合考虑所有输入，输出对应个数的结果  
- 每一个输出都考虑到所有输入的资讯
- self-attention和FC交替使用
- self attention之前就有人提出类似的机制，但是不叫这个名字，在[\[1706.03762\] Attention Is All You Need](https://arxiv.org/abs/1706.03762) 中正式提出，同时提出了Transformer

- **为什么要有自注意力机制**？
	- **找向量之间的关联性**
![Pasted image 20240629102644|400](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240629102644.png)

- **注意力怎么算**
	- **Dot-product** 点乘  
![Pasted image 20240629103029|343](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240629103029.png)
假设只有两个输入，给定两个输入，一个作为query，一个作为key  
与两个权重矩阵相乘，再进行点乘  


##### 具体怎么做  
![Pasted image 20240629104235|500](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240629104235.png)
输入向量$$a_1,a_2,a_3, a_4.....$$  

**token**  

取一个向量做query  
剩下的向量key都与它算注意力   
$$q^1 = W^qa^1$$
$$k^2 = W^ka^2$$
$$k^3 = W^ka^3$$
$$k^4 = W^ka^4$$
> 上标是第几个

- q与每个k计算注意力分数 attention score
$$a_{1,i} = q^1·k^i$$


实际上会和自己做关联性  (效果比较好)
$$a_{1,1} = q^1·k^1$$

然后过一层softmax  
- 更像是归一化  e
$$a_{1,i}' = \frac{exp(a_{1,i})}{\sum_j^Nexp(a_{1,j})}$$
![Pasted image 20240629110514|550](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240629110514.png)


再然后抽取重要的资讯  
![Pasted image 20240629110042|550](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240629110042.png)  
原向量再乘以一个矩阵，计算没加入注意力分数的原本输出
- 如果没有自注意力机制，直接过FC的话就是一个加权和
- 在这里就是计算原来不考虑相关度的加权分数
- 在这样的基础上再根据注意力分数做一次加权
$$v^i = W^va^i$$

每个$v^i$都与分数相乘，加和得到$a^1$ 过自注意力之后的输出 $b^1$  
$$b^1 = \sum_ia_{1,i}'v_i$$
- 如果和$a^1$ 的关联度很高的话，注意力分数就会很高，$v^i$和注意力分数相乘就会比较大，在$b^1$中占比就大，$b^1$就体现了比较多的它的信息   
- $b^{1-n}$是一起算的，没有顺序的概念
###### 小总结
到这里，self-attention有三部分
- 计算关联度/注意力分数（dot-product）
- 过一层softmax
- 提取重要资讯(相乘加和)

几个点
- 单位是向量，输入是多个向量
- 对向量内部的加权是通过矩阵乘法实现的
- 原本只有计算v然后加权和(只有FC的话)
- 现在权重改成注意力分数的归一化

###### 矩阵的话
![Pasted image 20240630100136|480](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240630100136.png)
- 每一个a都得进行一次矩阵乘法计算query 
- 每一个a都得进行一次矩阵乘法计算key
- 同理v也是
所以如果用矩阵的角度  
计算query，key，v  
$$Q = W^qI$$
$$K = W^kI$$
$$V = W^vI$$
计算注意力分数    
![Pasted image 20240630100930](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240630100930.png)
$$K^T\textbf{q}^i = \textbf{a}_{i}$$

则计算所有注意力分数      
$$K^TQ = A$$
然后归一化  
$$softmax(A) = A'$$
- $A'$称Attention Matric

计算最终分数  
$$O = VA'$$

所以全程就是一套矩阵乘法，只有三个权重矩阵是未知的(学习的)  

###### Multi-head self-attention  

可能会有不同类型的相关度(different type of relevance)  
- 需要多个query和多个key确定不同类型的相关度  
- 需要多个v来计算输出  
![Pasted image 20240701084105|600](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240701084105.png)   
- 然后照常计算(分开算  
![Pasted image 20240701084304|550](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240701084304.png)
- 然后转化输出 
![Pasted image 20240701084339|491](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240701084339.png)
###### Positional Encoding
到这里，注意力的计算和位置无关，完全就是两个向量本身的相关度，只要是相同的向量，无论出现在向量序列的哪里算出来的注意力分数都一样  

加上一个位置的偏移  
- 为每一个位置配备一个偏移向量---位置向量(positional vector) $e^i$
- 然后每次这个位置的输入都加上这个偏移
- ![Pasted image 20240701090055|351](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240701090055.png)

#### Encoder
![Pasted image 20240416162315](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240416162315.png)

实际的Transformer中，self-attention之后，还将input加到output中——**residual connection**    
![Pasted image 20240416162822](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240416162822.png)   
然后对**a+b**进行**layer normalization**  
![Pasted image 20240416162947](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240416162947.png)   

对输入计算mean和standard deviation，做归一化  
![Pasted image 20240416163228](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240416163228.png)  

#### Decoder  
![Pasted image 20240416165138|](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240416165138.png)
- **special token**作为开始
	- 用one-hot vector 表示，一维1，其他0 
- 吃到begin后，输出一个**向量**  
	- 和vocabulary(希望输出的内容)一样大 （例如常见的3000千方块字）
	- 输出的向量会进行一次[[softmax]]，得到各个token的**分数**，最高最为输出。

![Pasted image 20240416165504](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240416165504.png)
- 上一个输出不断加到输入中
	- 表示为one-hot vector
	- 前面均作为输入？
	- **其实encoder也有输入**
- 由于decoder将自己的输出作为输入，所以有可能会有**error propagation**的问题
###### 那怎么停止
- 直到输出special token 作为**END**

###### Masked self attention
![Pasted image 20240416174950](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240416174950.png)
- 原本就是一个个产生的，只能考虑已经生成的部分  
- 所以需要把后面的都掩藏起来，称为sequence mask
- 此外还有padding mask

#### Encoder-Decoder
![Pasted image 20240416181240](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240416181240.png)

将输入喂给Encoder，给Decoder一个Begin，Encoder输出后(多个向量)与Decoder第一次归一化后的输出(一个向量)进入self-attention计算attention分数。  
![Pasted image 20240416183053|400](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240416183053.png)

中间计算attention的过程称**cross attention**  
通过第一个q，decoder从什么都没有去encoder中获得信息，信息不断迭代进入下一轮。  
实际上decoder很多层，每一层都拿了encoder最后一层的输出。  


## API 参数

- API参数
	- OpenAI为例
		- `system`: 固定的prompt
		- `user`：用户输入
		- `assistant`：模型的回答
	- 各个参数
		- `max_tokens`：最大的token数量
		- `temperature`：温度
		- `top_p`：分布函数
		- `n`：n个回答
		- `stop`：指明停止的token
		- `tools`：工具（函数）的说明
		- `logprobs`：是否返回每个token的概率
		- `logit_bias`：对某个词的概率偏差
	- function call
		- 大概的流程
		- ![Pasted image 20240711110318](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240711110318.png)

### 关于softmax
softmax是一个归一化函数，接受实数向量，计算出对应的归一化的概率向量，并保证每个数都大于或等于0。  


softmax 定义式是：
$$softmax(x_i) = \frac{e^{x_i}}{\sum_{j=1}^ne^{x_j}}$$

粗略的看，softmax是one-hot(argmax)的光滑近似  
argmax在一个向量中取出最大值所在的位置，one-hot(i)生成第i位为1，其余为0的向量 

##### 详见
- [函数光滑近似（2）：softmax与argmax | Erwin Feng Blog](https://allenwind.github.io/blog/9905/)
- [分析与拓展：多分类模型的输出为什么使用softmax？ | Erwin Feng Blog](https://allenwind.github.io/blog/15110/)
### Sampling
pre:
- softmax
aim at:
- temperature sample
- nucleus sample (top-p)
- top-k

语言模型的输出(GPT-2)是通过自回归的方式输出的，将当前输出的token迭代进行下一次的预测，计算分布，得到概率输出。但是这样的输出经常出现问题(stuck or loop)  
- 如果只选最优概率，可能导致stuck and loop（注意力集中在后几个）
- 取样会更好
- 随机取可能会有很大一部分token和答案毫不相关，但是这部分token占有概率，被选中有可能导致推理脱轨
- 为了避免选中这些东西，流行的方法有temperature sampling and top k sampling

##### Temperature sampling
inspired by statistical thermodynamics, where high temperature means low energy states are more likely encountered.  低能量的状态遇到的概率更大  

- in probability models, logits(the raw output of the final linear layer)
- before feeding to softmax, implement temperature sampling by dividing logits by the temperature.
- **高温下，经过softmax，低概率的数的概率相对增大**
```python
import torch
import torch.nn.functional as F
temperature = 0.5  # ex
a = torch.tensor([1,2,3,4])
F.softmax(a/temperature, dim=0)
```

- in a conclusion
  Lower temperatures make the model increasingly confident in its top choices, while temperatures greater than 1 decrease confidence. 
- 温度越高，随机性越大

- addition
  0 temperature is equivalent to argmax/max likelihood, while infinite temperature corresponds to a uniform sampling.
###### 为什么相对增大?
softmax 定义式是：
$$softmax(x_i) = \frac{e^{x_i}}{\sum_{j=1}^ne^{x_j}}$$
加上$\tau$ (temperature)
$$softmax(x_i) = \frac{e^{x_i/\tau}}{\sum_{j=1}^ne^{x_j/\tau}}$$

- 对于softmax的值来说，$x/\tau$ ，x越大，对于$e^x$来说变化的幅度就越大(指数曲线)，所有$x_i$在x上的放大（缩小）倍数是一样的，原来概率大的token softmax出来之后，分子分母变化都大，而概率小的分母变化更大。
- 当温度低，概率小的token分母增大相对分子更明显，概率随温度低而减小
- 当温度高，概率小的token分母减小相对分子更明显，数值增大，而概率大的相反

所以$\tau$越大，低概率token抢占的概率更大，随机性越高。


##### Top k sampling
**sorting by probability and zeroing out the probabilities for anything below the k'th token**
除了前k个外都置为0  

- it appears to improve the result
- but in some cases, the probabilities of token can not be aparently seperated
- 可能多个token都有道理，概率分布的差异并不显著
- propose nucleus(/ˈnjuː.kli.əs/) sampling
##### nucleus sample
- top-p sampling, where p is the threshold of the CDF.
- calculate the first n token and cut off as soon as CDF exceed p
- 保留了一定的随机性

##### Why maximum likelihood may not work?
- it can never see the errors inside the model, it may predict a error token but use "correct" method for the next one, expanding the error.
- 或者，当只选取概率最大时，由于注意力只集中在后几个上，所以算出来可能会是单考虑特定token时的答案，此时会导向不合理的结果

##### 取样之后呢
- top-p top-k temperature 都是选出区间
- temperature 和 top-k top-p可以一起使用
- 在这之后根据概率取样

根据karpathy的手搓LLama2  
- 可以是设定一个0到1的随机数(coin)，将概率倒序排列
- 当分布函数CDF 取值超过 coin时，选出此时最后一个加入的token
- coin的选定是随机的
- coin落到哪里概率都一样，那coin是否落在一个token的概率范围里只和概率的大小有关，实现了概率取样，得到真正的next token
```c
int sample_mult(float* probabilities, int n, float coin) {
    // sample index from probabilities (they must sum to 1!)
    // coin is a random number in [0, 1), usually from random_f32()
    float cdf = 0.0f;
    for (int i = 0; i < n; i++) {
        cdf += probabilities[i];
        if (coin < cdf) {
            return i;
        }
    }
    return n - 1; // in case of rounding errors
}
```

对于top-p来说，最后的计算只需限制前k个里面选，即在概率为cdf里面而不是在1里  

```c
int sample_topp(float* probabilities, int n, float topp, ProbIndex* probindex, float coin) {
    // top-p sampling (or "nucleus sampling") samples from the smallest set of
    // tokens that exceed probability topp. This way we never sample tokens that
    // have very low probabilities and are less likely to go "off the rails".
    // coin is a random number in [0, 1), usually from random_f32()

    int n0 = 0;
    // quicksort indices in descending order of probabilities
    // values smaller than (1 - topp) / (n - 1) cannot be part of the result
    // so for efficiency we crop these out as candidates before sorting
    const float cutoff = (1.0f - topp) / (n - 1);
    for (int i = 0; i < n; i++) {
        if (probabilities[i] >= cutoff) {
            probindex[n0].index = i;
            probindex[n0].prob = probabilities[i];
            n0++;
        }
    }
    qsort(probindex, n0, sizeof(ProbIndex), compare);

    // truncate the list where cumulative probability exceeds topp
    float cumulative_prob = 0.0f;
    int last_idx = n0 - 1; // in case of rounding errors consider all elements
    for (int i = 0; i < n0; i++) {
        cumulative_prob += probindex[i].prob;
        if (cumulative_prob > topp) {
            last_idx = i;
            break; // we've exceeded topp by including last_idx
        }
    }

    // sample from the truncated list
    float r = coin * cumulative_prob;
    float cdf = 0.0f;
    for (int i = 0; i <= last_idx; i++) {
        cdf += probindex[i].prob;
        if (r < cdf) {
            return probindex[i].index;
        }
    }
    return probindex[last_idx].index; // in case of rounding errors
}
```

## Reference
- [李宏毅【機器學習2021】Transformer (上) - YouTube](https://www.youtube.com/watch?v=n9TlOhRjYoc&t=60s)
- [李宏毅【機器學習2021】Transformer (下) - YouTube](https://www.youtube.com/watch?v=N6aRv06iv2g&t=1574s)
- [李沐Transformer论文逐段精读 - YouTube](https://www.youtube.com/watch?v=nzqlFIcCSWQ) 

