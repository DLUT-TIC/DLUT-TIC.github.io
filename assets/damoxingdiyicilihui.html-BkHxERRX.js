import{_ as a,r as t,o as s,c as n,b as e,d as l,a as r,e as o}from"./app-Y5F1Jwa1.js";const p={},c=e("h1",{id:"andrej-karpathy-intro-to-large-language-models",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#andrej-karpathy-intro-to-large-language-models"},[e("span",null,"(Andrej Karpathy) Intro to Large Language Models")])],-1),m=e("blockquote",null,[e("p",null,"批注或想法以这种方式记录")],-1),d=e("blockquote",null,[e("p",null,"[!notation] 这种形式用于概念的补充说明")],-1),g=e("h2",{id:"from",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#from"},[e("span",null,"From")])],-1),h={href:"https://www.youtube.com/watch?v=zjkBMFhNj_g",target:"_blank",rel:"noopener noreferrer"},u=o('<h2 id="intro-to-llm" tabindex="-1"><a class="header-anchor" href="#intro-to-llm"><span>Intro to LLM</span></a></h2><h3 id="what-is-llm" tabindex="-1"><a class="header-anchor" href="#what-is-llm"><span>What is LLM</span></a></h3><p>以llama2-70b为例<br><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240708170432.png" alt="Pasted image 20240708170432|398"></p><ul><li>llama 是模型名字</li><li>2 是指这是该模型的第二代</li><li>70b 是模型参数大小 (b-billion)</li></ul><h4 id="_2-files" tabindex="-1"><a class="header-anchor" href="#_2-files"><span>2 files</span></a></h4><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240607000442.png" alt="Pasted image 20240607000442|300"></p><ul><li>the parameters files (for the nerual network)</li><li>the code to run the parameters In LLama2-70B, each parameter stored as 2 bytes(float16 number) and so therefore the parameters file is 140,000,000,000 Bytes</li></ul><blockquote><p>[!float16] 什么是float16 👈详见机组 十六位的浮点数数字格式 -- 半精度浮点数(FP16/float16) 相比于32位双精度，精度低，但是更小更快 常用于存储不要求高精度的浮点值，例如图像处理和神经网络 <img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240707153315.png" alt="Pasted image 20240707153315|400"></p></blockquote><p>大模型是参数+运行这些参数的代码，只要满足这两个条件就能运行大模型而不需要任何魔法</p><h4 id="obtain-the-parameters" tabindex="-1"><a class="header-anchor" href="#obtain-the-parameters"><span>Obtain the parameters</span></a></h4><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240707154310.png" alt="Pasted image 20240707154310|550"></p><ul><li>a chunk of the internet --- for llama2 10TB of text</li><li>GPU cluster --- 6000GPU</li><li>10TB fo text - 6000GPUs - 12days for llama2-70b 2million dollars</li><li>generate 140GB parameters.zip file</li><li>可以这么理解，训练过程是把10TB的知识进行压缩，最终得到一个压缩包(参数)</li><li><strong>BUT</strong> zip is a lossless compression, but here is a <strong>lossy compression</strong></li><li>100x</li><li>通常表现更好的模型需要更多的数据和GPU和训练时间和钱</li></ul><h4 id="function-of-parameters" tabindex="-1"><a class="header-anchor" href="#function-of-parameters"><span>function of parameters</span></a></h4><p><strong>predict the next word</strong><br> give some words and it give you the next word<br><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240707154945.png" alt="Pasted image 20240707154945|475"></p><p>很多人因此认为大模型就是在做词语接龙而已，不具备真正的推理能力。</p><p>值得一提的是 当预测下一个词的时候，实际上需要将大量世界知识压缩到权重中，可能并不只是token接龙<br><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240707155445.png" alt="Pasted image 20240707155445|550"></p><ul><li>如果大模型生成这样的文章</li><li>在每一个词之后实际上有很多种可能，大模型需要知道宝可梦的发布的具体时间，需要知道创始人，需要知道2016年有pokemon go的发布</li><li>不可否认完成这项任务的大模型拥有了广泛的世界知识</li></ul><p><strong>the networks dreams internet documents</strong><br> 看上去是正确的，但难以确保哪些是对的哪些是不对的，概率上只是输出符合要求的形式，内容像dreaming</p><p>那你需要知道的</p><ul><li>大模型通过概率计算生成下一个词</li><li>概率是神经网络得到的</li><li>生成的东西不一定正确，不正确的称为幻觉 (hallucination /həˌluː.sɪˈneɪ.ʃən/)</li><li>大模型确实拥有世界知识（世界知识被压缩在大模型的参数之中）</li></ul><h3 id="how-does-it-work" tabindex="-1"><a class="header-anchor" href="#how-does-it-work"><span>How does it work?</span></a></h3><p><strong>Transformer structure</strong><br><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240708170512.png" alt="Pasted image 20240708170512|425"><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240707160436.png" alt="Pasted image 20240707160436|428"></p><ul><li>每一层的数学原理都是可知的</li><li>但是权重(参数)分布在整个神经网络中</li><li>所以我们只能知道怎么调整这些参数(作为一个整体)，来改善模型的表现</li><li>没办法知道哪一个参数具体在做什么</li></ul><blockquote><p>[!可解释性] 模型的可解释性 有专门研究可解释性的领域，尝试深入黑盒探寻各个参数的行为 现在并不完善，如果有兴趣可以进一步研究</p></blockquote><p>可以将这些权重看成是建立了一个奇怪的不完美的知识库</p><ul><li>reversal curse</li><li><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240608213353.png" alt="Pasted image 20240608213353|286"></li><li>知识像是一维的，而不是确切地存有知识，能从各个方向访问，只能是单向</li><li><strong>empirical 经验主义</strong></li></ul><blockquote><p>[!经验论和唯理论的争辩] 人工智能有两派观点<br> 符号主义和联结主义</p><ul><li>符号主义主张用符号描述世界</li><li>联结主义利用神经网络的连接机制来模仿人类</li><li>早期人工智能学者大多是符号主义，现在联结主义兴盛</li></ul><p>不是很正确的说，符号主义接近一种唯理论，得出规范统一的理论后在理论的指导下进行决策，联结主义(深度神经网络)接近一种经验论，收集大量数据，从数据(经验)中获取知识，这一点在大模型上尤为突出 将神经网络和符号方法结合是一个新的方向，大模型现在也在做</p></blockquote><h3 id="two-stages-training" tabindex="-1"><a class="header-anchor" href="#two-stages-training"><span>Two stages training</span></a></h3><p>Internet document generators阶段，称预训练，第一阶段 fine tuning 如果有的话，为第二阶段 <strong>obtain assistant model</strong></p><ul><li>keep the trainning the same -- next word prediction</li><li>modify the data set <ul><li>一般是手工收集数据 (找一堆廉价劳动力，或者使用大模型)</li><li>为特定的数据打标签</li><li><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240608215345.png" alt="Pasted image 20240608215345|272"> 微调后，模型使用assistant的方式输出，但仍然能使用前一阶段的训练得到的知识，同样难以被解释</li></ul></li></ul><h3 id="procedure" tabindex="-1"><a class="header-anchor" href="#procedure"><span>Procedure</span></a></h3><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240609003034.png" alt="Pasted image 20240609003034|400"></p><p><strong>第一阶段称为pre-trainning, about knowledge</strong> --- base model <strong>第二阶段称为fine-tuning, about alignment</strong> --- assistant model</p><blockquote><p>第一阶段的训练基本决定了模型的表现，第二阶段在小规模数据下更接近于学习某种回答方式 但是有通过第二阶段获取专有能力模型的情况(医疗模型，化学模型，金融模型...)</p></blockquote><p>所以大模型大概是这么生产的</p><ol><li>预训练模型(十几天一两个月)</li><li>微调(一两天)</li><li>测试，找出有问题的回答</li><li>解决这些问题</li></ol><p><strong>怎么解决问题</strong></p><ul><li>微调后得到的模型出错的地方，人为修正，整合到数据集然后再train</li><li>迭代过程</li></ul><blockquote><p>[!base model &amp; assistant model]</p><ul><li>base model is not super helpful, it may not answer question with answers We can do our own fine tuning on it</li><li>assitant model question answer</li></ul></blockquote><h3 id="second-kind-of-label" tabindex="-1"><a class="header-anchor" href="#second-kind-of-label"><span>Second kind of label</span></a></h3><p>In fine-tuning stage, step 2 and 3<br><strong>comparison labels</strong><br> 比较哪个label好要比直接写答案简单<br><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240609003341.png" alt="Pasted image 20240609003341|375"> 通过对比模型输出，选择一个最好的加入训练集<br> openai称为：<strong>RLHF</strong> reinforcement learning from human feedback 通过人类反馈来进行强化学习</p><p><strong>现实中label逐渐变成人和模型协同的工作</strong></p><h2 id="improvement" tabindex="-1"><a class="header-anchor" href="#improvement"><span>Improvement</span></a></h2><h4 id="scaling-laws" tabindex="-1"><a class="header-anchor" href="#scaling-laws"><span>scaling laws:</span></a></h4><ul><li><strong>Performance of LLMs is a smooth, well-behaved, predictable function</strong></li><li>with 2 varibles: <ul><li><strong>N - the number of parameters in the network</strong> 参数量</li><li><strong>D - the amount of text we train on</strong> 训练的文本量 <strong>only these two numbers can predict what accuracy LLM will achieve on the next word prediction task</strong><br> and the trend <strong>do not show signs of topping out</strong><br> 不会到达顶点<br> algorithm is not that necessary(as nice bonus), but we can just get better model for free with better computer</li></ul></li></ul><blockquote><p>力大砖飞！</p></blockquote><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240609005827.png" alt="Pasted image 20240609005827|525"></p><h4 id="evolving" tabindex="-1"><a class="header-anchor" href="#evolving"><span>Evolving</span></a></h4><p>agent</p><p>以chatGPT为例<br> 实际上使用时不是完全由模型生成，会使用工具</p><ul><li>web</li><li>计算器</li><li>代码</li></ul><p>using tools and infrastructures, tying everything together</p><blockquote><p>藏在Web背后的大模型基本就是一个工具集合体，agent的路是不是已经被大企业们遥遥领先</p></blockquote><h2 id="future" tabindex="-1"><a class="header-anchor" href="#future"><span>Future</span></a></h2><h4 id="system-2" tabindex="-1"><a class="header-anchor" href="#system-2"><span>System 2</span></a></h4><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240609013341.png" alt="Pasted image 20240609013341|400"></p><p>可以认为，人类有两种思考方式</p><ul><li>一种是system1，自动的，快速的，无需过多思考的，感情化的，直觉的等等 <ul><li>speed chess</li></ul></li><li>一种是system2，理性，慢 <ul><li>competition</li></ul></li></ul><p>现在的大模型基本是第一种</p><blockquote><p>大模型是一个函数，给定输入产生输出，没有为自己的“思考”进行处理(深层思考)，有一些工作真在改善这一点，如 CoT，ToT等方法。</p></blockquote><h4 id="self-improvement" tabindex="-1"><a class="header-anchor" href="#self-improvement"><span>self-improvement</span></a></h4><ul><li>AlphaGo <ul><li>learn by imitation, can&#39;t supass human</li><li>self improvement, with a simple reward function (win)</li></ul></li></ul><p>对于大模型来说，难以确认奖励和惩罚<br><strong>在小的领域内可行</strong></p><h4 id="custom" tabindex="-1"><a class="header-anchor" href="#custom"><span>Custom</span></a></h4><p>为每一个领域训练一个专门的模型<br> 或<br> RAG</p><p>GPTs &lt;- 但是GPTs没有做起来 众多国内的乱七八糟助手</p><h4 id="llm-os" tabindex="-1"><a class="header-anchor" href="#llm-os"><span>LLM OS</span></a></h4><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240609014639.png" alt="Pasted image 20240609014639|425"> as kernel of a emerging operating system</p><p>将现有的都合起来</p><ul><li>对话能力</li><li>RAG个人语料检索</li><li>多工具使用</li><li>多模态，语音，图片，视频</li><li>微调形式上的优化</li><li>个性化设计</li><li>小领域的self-improvement</li></ul><p>multi-threading, multi-processing, speculative execution(预测执行), user space, kernel space 都能在LLM OS中找到对应抽象</p><p>除此之外：</p><ul><li>主流系统与其他系统构成的庞大生态</li><li><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240609015613.png" alt="Pasted image 20240609015613"></li></ul><h2 id="security" tabindex="-1"><a class="header-anchor" href="#security"><span>Security</span></a></h2><h4 id="jailbreak-attacks" tabindex="-1"><a class="header-anchor" href="#jailbreak-attacks"><span>jailbreak attacks</span></a></h4><p>越狱攻击<br> 制作<strong>napalm</strong><br><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240609020024.png" alt="Pasted image 20240609020024|575"><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240609020107.png" alt="Pasted image 20240609020107|575"></p><ul><li><p>base64 encoding of the same query</p></li><li><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240609020224.png" alt="Pasted image 20240609020224|300"></p></li><li><p>可以认为是另一种语言，一些语言也可能产生问题</p><ul><li>安全问题大多基于英语</li><li>没有学会拒绝非安全query，只是学会了某种模式而已</li></ul></li><li><p>universal transferable suffix</p></li><li><p>训练出来的攻击模型的后缀</p></li><li><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240707211504.png" alt="Pasted image 20240707211504|500"></p></li><li><p>噪声处理(训练)出来的图片实现越狱</p></li><li><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240707211616.png" alt="Pasted image 20240707211616|525"></p></li></ul><h4 id="prompt-injection" tabindex="-1"><a class="header-anchor" href="#prompt-injection"><span>Prompt injection</span></a></h4><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240609021048.png" alt="Pasted image 20240609021048|375"></p><p>将prompt藏在人眼不可见的位置(白色字体)，当LLM看到这段prompt时，会忘记之前的prompt（取代原来的prompt），执行当前prompt</p><ul><li><p>将prompt injection attack放在网页里</p></li><li><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240609021327.png" alt="Pasted image 20240609021327|325"></p></li><li><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240707211839.png" alt="Pasted image 20240707211839|450"></p></li><li><p>放在文档里</p></li><li><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240609021657.png" alt="Pasted image 20240609021657|350"></p></li><li><p>当获得一份邮件，让bard总结时，内部有一个新prompt，提示大模型向特定的黑客控制的url发送请求，发送请求时会编码用户个人信息，数据泄露</p></li><li><p>谷歌工程师设置了权限来解决</p></li><li><p>然后出现了一个 Google Apps Scriptes，此时权限没有拒绝，可以将数据传送到Google doc，同时攻击者能够访问这个文件，再次泄露</p><ul><li>分享文件实现攻击</li></ul></li></ul><h4 id="data-poisoning-backdoor-attacks" tabindex="-1"><a class="header-anchor" href="#data-poisoning-backdoor-attacks"><span>Data poisoning/Backdoor attacks</span></a></h4><p><strong>trigger phrase</strong> triger LLM to perform specific prediction<br> 黑客通过在网络上设置一些文档，包含trigger phrase，使用这些文档进行训练的模型就会受影响<br> 下毒<br><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240609022325.png" alt="Pasted image 20240609022325|450"><strong>trigger words corrupt model</strong><br> 目前是在微调时有效</p><h4 id="总" tabindex="-1"><a class="header-anchor" href="#总"><span>总</span></a></h4><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240609022518.png" alt="Pasted image 20240609022518|375"></p><blockquote><p>大模型安全也是一个新兴领域，有兴趣从事这方面科研工作的可以多做了解</p></blockquote>',86);function b(f,w){const i=t("ExternalLinkIcon");return s(),n("div",null,[c,m,d,g,e("p",null,[e("a",h,[l("[1hr Talk] Intro to Large Language Models - YouTube"),r(i)])]),u])}const P=a(p,[["render",b],["__file","damoxingdiyicilihui.html.vue"]]),_=JSON.parse('{"path":"/docs/ai/damoxingdiyicilihui.html","title":"大模型第一次例会","lang":"zh-CN","frontmatter":{"title":"大模型第一次例会","author":"saber","date":"2024/9/9"},"headers":[{"level":2,"title":"From","slug":"from","link":"#from","children":[]},{"level":2,"title":"Intro to LLM","slug":"intro-to-llm","link":"#intro-to-llm","children":[{"level":3,"title":"What is  LLM","slug":"what-is-llm","link":"#what-is-llm","children":[]},{"level":3,"title":"How does it work?","slug":"how-does-it-work","link":"#how-does-it-work","children":[]},{"level":3,"title":"Two stages training","slug":"two-stages-training","link":"#two-stages-training","children":[]},{"level":3,"title":"Procedure","slug":"procedure","link":"#procedure","children":[]},{"level":3,"title":"Second kind of label","slug":"second-kind-of-label","link":"#second-kind-of-label","children":[]}]},{"level":2,"title":"Improvement","slug":"improvement","link":"#improvement","children":[]},{"level":2,"title":"Future","slug":"future","link":"#future","children":[]},{"level":2,"title":"Security","slug":"security","link":"#security","children":[]}],"git":{"createdTime":1725811061000,"updatedTime":1725811061000,"contributors":[{"name":"saber","email":"wuyacwc@gmail.com","commits":1}]},"filePathRelative":"docs/ai/大模型第一次例会.md"}');export{P as comp,_ as data};
