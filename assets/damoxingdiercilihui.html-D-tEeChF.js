import{_ as i,r as l,o as p,c as o,b as n,d as a,e,a as t}from"./app-Dd5cVqOR.js";const c={},r=t(`<blockquote><p>[!warning] 文档内容可能存在些许错误</p></blockquote><h2 id="前置条件" tabindex="-1"><a class="header-anchor" href="#前置条件"><span>前置条件</span></a></h2><h6 id="环境-推荐新建一个环境" tabindex="-1"><a class="header-anchor" href="#环境-推荐新建一个环境"><span>环境（推荐新建一个环境）</span></a></h6><ul><li>一个虚拟环境</li></ul><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="line">conda create <span class="token parameter variable">-n</span> yourname</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li><strong>jupyter lab</strong> （如果有装anaconda的话已经安装好了）</li><li>将新的虚拟环境加入notebook中</li></ul><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="line"><span class="token comment"># 在base环境下</span></span>
<span class="line">conda activate</span>
<span class="line">conda <span class="token function">install</span> nb_conda_kernels</span>
<span class="line"></span>
<span class="line"><span class="token comment"># 在要打开的环境</span></span>
<span class="line">conda activate your_env_name</span>
<span class="line">conda <span class="token function">install</span> ipykernel</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h6 id="依赖项" tabindex="-1"><a class="header-anchor" href="#依赖项"><span>依赖项</span></a></h6><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre class="language-python"><code><span class="line">pip install <span class="token operator">-</span><span class="token operator">-</span>upgrade openai</span>
<span class="line">pip <span class="token builtin">list</span>  <span class="token comment"># 查看有没有成功安装</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><h6 id="注册moonshot-api" tabindex="-1"><a class="header-anchor" href="#注册moonshot-api"><span>注册moonshot API</span></a></h6>`,10),u=n("s",null,"token免费送！",-1),d={href:"https://platform.moonshot.cn/console/account",target:"_blank",rel:"noopener noreferrer"},m=t('<h2 id="大概纲要" tabindex="-1"><a class="header-anchor" href="#大概纲要"><span>大概纲要</span></a></h2><ul><li>Transformer <ul><li>Seq2Seq</li><li>self-attention</li><li>Encoder</li><li>Decoder <ul><li>softmax</li></ul></li><li>Encoder-decoder</li></ul></li><li>API参数 <ul><li>OpenAI为例</li></ul></li></ul><h2 id="transformer" tabindex="-1"><a class="header-anchor" href="#transformer"><span>Transformer</span></a></h2><p>^48eae8</p><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240712122336.png" alt="Pasted image 20240712122336"></p><h4 id="seq2seq" tabindex="-1"><a class="header-anchor" href="#seq2seq"><span>Seq2Seq</span></a></h4><p><strong>Seq2Seq</strong>（Sequence to Sequence，序列到序列模型）<br> 包括<strong>编码器(Encoder)</strong>，<strong>解码器(Decoder)</strong> 两部分。<br> Seq2Seq 输出不定长度的序列</p><blockquote><p>[!什么可以是不定长度的序列] 文字 语音(音频)</p><ul><li>语音识别，语音翻译，聊天 图片</li><li>拆解图像，RGB三维向量</li></ul></blockquote><h4 id="self-attention" tabindex="-1"><a class="header-anchor" href="#self-attention"><span>self-attention</span></a></h4><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240629101948.png" alt="Pasted image 20240629101948|500"> 整个序列(向量组)输入，综合考虑所有输入，输出对应个数的结果</p>',10),k=n("li",null,[n("p",null,"每一个输出都考虑到所有输入的资讯")],-1),h=n("li",null,[n("p",null,"self-attention和FC交替使用")],-1),b={href:"https://arxiv.org/abs/1706.03762",target:"_blank",rel:"noopener noreferrer"},g=t('<li><p><strong>为什么要有自注意力机制</strong>？</p><ul><li><strong>找向量之间的关联性</strong><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240629102644.png" alt="Pasted image 20240629102644|400"></li></ul></li><li><p><strong>注意力怎么算</strong></p><ul><li><strong>Dot-product</strong> 点乘<br><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240629103029.png" alt="Pasted image 20240629103029|343"> 假设只有两个输入，给定两个输入，一个作为query，一个作为key<br> 与两个权重矩阵相乘，再进行点乘</li></ul></li>',2),v=t('<h5 id="具体怎么做" tabindex="-1"><a class="header-anchor" href="#具体怎么做"><span>具体怎么做</span></a></h5><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240629104235.png" alt="Pasted image 20240629104235|500"> 输入向量$$a_1,a_2,a_3, a_4.....$$</p><p><strong>token</strong></p><p>取一个向量做query<br> 剩下的向量key都与它算注意力<br> $$q^1 = W^qa^1$$ $$k^2 = W^ka^2$$ $$k^3 = W^ka^3$$ $$k^4 = W^ka^4$$</p><blockquote><p>上标是第几个</p></blockquote><ul><li>q与每个k计算注意力分数 attention score $$a_{1,i} = q^1·k^i$$</li></ul><p>实际上会和自己做关联性 (效果比较好) $$a_{1,1} = q^1·k^1$$</p><p>然后过一层softmax</p><ul><li>更像是归一化 e $$a_{1,i}&#39; = \\frac{exp(a_{1,i})}{\\sum_j^Nexp(a_{1,j})}$$ <img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240629110514.png" alt="Pasted image 20240629110514|550"></li></ul><p>再然后抽取重要的资讯<br><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240629110042.png" alt="Pasted image 20240629110042|550"><br> 原向量再乘以一个矩阵，计算没加入注意力分数的原本输出</p><ul><li>如果没有自注意力机制，直接过FC的话就是一个加权和</li><li>在这里就是计算原来不考虑相关度的加权分数</li><li>在这样的基础上再根据注意力分数做一次加权 $$v^i = W^va^i$$</li></ul><p>每个$v^i$都与分数相乘，加和得到$a^1$ 过自注意力之后的输出 $b^1$<br> $$b^1 = \\sum_ia_{1,i}&#39;v_i$$</p><ul><li>如果和$a^1$ 的关联度很高的话，注意力分数就会很高，$v^i$和注意力分数相乘就会比较大，在$b^1$中占比就大，$b^1$就体现了比较多的它的信息</li><li>$b^{1-n}$是一起算的，没有顺序的概念</li></ul><h6 id="小总结" tabindex="-1"><a class="header-anchor" href="#小总结"><span>小总结</span></a></h6><p>到这里，self-attention有三部分</p><ul><li>计算关联度/注意力分数（dot-product）</li><li>过一层softmax</li><li>提取重要资讯(相乘加和)</li></ul><p>几个点</p><ul><li>单位是向量，输入是多个向量</li><li>对向量内部的加权是通过矩阵乘法实现的</li><li>原本只有计算v然后加权和(只有FC的话)</li><li>现在权重改成注意力分数的归一化</li></ul><h6 id="矩阵的话" tabindex="-1"><a class="header-anchor" href="#矩阵的话"><span>矩阵的话</span></a></h6><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240630100136.png" alt="Pasted image 20240630100136|480"></p><ul><li>每一个a都得进行一次矩阵乘法计算query</li><li>每一个a都得进行一次矩阵乘法计算key</li><li>同理v也是 所以如果用矩阵的角度<br> 计算query，key，v<br> $$Q = W^qI$$ $$K = W^kI$$ $$V = W^vI$$ 计算注意力分数<br><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240630100930.png" alt="Pasted image 20240630100930"> $$K^T\\textbf{q}^i = \\textbf{a}_{i}$$</li></ul><p>则计算所有注意力分数<br> $$K^TQ = A$$ 然后归一化<br> $$softmax(A) = A&#39;$$</p><ul><li>$A&#39;$称Attention Matric</li></ul><p>计算最终分数<br> $$O = VA&#39;$$</p><p>所以全程就是一套矩阵乘法，只有三个权重矩阵是未知的(学习的)</p><h6 id="multi-head-self-attention" tabindex="-1"><a class="header-anchor" href="#multi-head-self-attention"><span>Multi-head self-attention</span></a></h6><p>可能会有不同类型的相关度(different type of relevance)</p><ul><li>需要多个query和多个key确定不同类型的相关度</li><li>需要多个v来计算输出<br><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240701084105.png" alt="Pasted image 20240701084105|600"></li><li>然后照常计算(分开算<br><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240701084304.png" alt="Pasted image 20240701084304|550"></li><li>然后转化输出 <img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240701084339.png" alt="Pasted image 20240701084339|491"></li></ul><h6 id="positional-encoding" tabindex="-1"><a class="header-anchor" href="#positional-encoding"><span>Positional Encoding</span></a></h6><p>到这里，注意力的计算和位置无关，完全就是两个向量本身的相关度，只要是相同的向量，无论出现在向量序列的哪里算出来的注意力分数都一样</p><p>加上一个位置的偏移</p><ul><li>为每一个位置配备一个偏移向量---位置向量(positional vector) $e^i$</li><li>然后每次这个位置的输入都加上这个偏移</li><li><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240701090055.png" alt="Pasted image 20240701090055|351"></li></ul><h4 id="encoder" tabindex="-1"><a class="header-anchor" href="#encoder"><span>Encoder</span></a></h4><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240416162315.png" alt="Pasted image 20240416162315"></p><p>实际的Transformer中，self-attention之后，还将input加到output中——<strong>residual connection</strong><br><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240416162822.png" alt="Pasted image 20240416162822"><br> 然后对<strong>a+b</strong>进行<strong>layer normalization</strong><br><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240416162947.png" alt="Pasted image 20240416162947"></p><p>对输入计算mean和standard deviation，做归一化<br><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240416163228.png" alt="Pasted image 20240416163228"></p><h4 id="decoder" tabindex="-1"><a class="header-anchor" href="#decoder"><span>Decoder</span></a></h4><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240416165138.png" alt="Pasted image 20240416165138|"></p><ul><li><strong>special token</strong>作为开始 <ul><li>用one-hot vector 表示，一维1，其他0</li></ul></li><li>吃到begin后，输出一个<strong>向量</strong><ul><li>和vocabulary(希望输出的内容)一样大 （例如常见的3000千方块字）</li><li>输出的向量会进行一次[[softmax]]，得到各个token的<strong>分数</strong>，最高最为输出。</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240416165504.png" alt="Pasted image 20240416165504"></p><ul><li>上一个输出不断加到输入中 <ul><li>表示为one-hot vector</li><li>前面均作为输入？</li><li><strong>其实encoder也有输入</strong></li></ul></li><li>由于decoder将自己的输出作为输入，所以有可能会有<strong>error propagation</strong>的问题</li></ul><h6 id="那怎么停止" tabindex="-1"><a class="header-anchor" href="#那怎么停止"><span>那怎么停止</span></a></h6><ul><li>直到输出special token 作为<strong>END</strong></li></ul><h6 id="masked-self-attention" tabindex="-1"><a class="header-anchor" href="#masked-self-attention"><span>Masked self attention</span></a></h6><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240416174950.png" alt="Pasted image 20240416174950"></p><ul><li>原本就是一个个产生的，只能考虑已经生成的部分</li><li>所以需要把后面的都掩藏起来，称为sequence mask</li><li>此外还有padding mask</li></ul><h4 id="encoder-decoder" tabindex="-1"><a class="header-anchor" href="#encoder-decoder"><span>Encoder-Decoder</span></a></h4><p><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240416181240.png" alt="Pasted image 20240416181240"></p><p>将输入喂给Encoder，给Decoder一个Begin，Encoder输出后(多个向量)与Decoder第一次归一化后的输出(一个向量)进入self-attention计算attention分数。<br><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240416183053.png" alt="Pasted image 20240416183053|400"></p><p>中间计算attention的过程称<strong>cross attention</strong><br> 通过第一个q，decoder从什么都没有去encoder中获得信息，信息不断迭代进入下一轮。<br> 实际上decoder很多层，每一层都拿了encoder最后一层的输出。</p><h2 id="api-参数" tabindex="-1"><a class="header-anchor" href="#api-参数"><span>API 参数</span></a></h2><ul><li>API参数 <ul><li>OpenAI为例 <ul><li><code>system</code>: 固定的prompt</li><li><code>user</code>：用户输入</li><li><code>assistant</code>：模型的回答</li></ul></li><li>各个参数 <ul><li><code>max_tokens</code>：最大的token数量</li><li><code>temperature</code>：温度</li><li><code>top_p</code>：分布函数</li><li><code>n</code>：n个回答</li><li><code>stop</code>：指明停止的token</li><li><code>tools</code>：工具（函数）的说明</li><li><code>logprobs</code>：是否返回每个token的概率</li><li><code>logit_bias</code>：对某个词的概率偏差</li></ul></li><li>function call <ul><li>大概的流程</li><li><img src="https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted image 20240711110318.png" alt="Pasted image 20240711110318"></li></ul></li></ul></li></ul><h3 id="关于softmax" tabindex="-1"><a class="header-anchor" href="#关于softmax"><span>关于softmax</span></a></h3><p>softmax是一个归一化函数，接受实数向量，计算出对应的归一化的概率向量，并保证每个数都大于或等于0。</p><p>softmax 定义式是： $$softmax(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^ne^{x_j}}$$</p><p>粗略的看，softmax是one-hot(argmax)的光滑近似<br> argmax在一个向量中取出最大值所在的位置，one-hot(i)生成第i位为1，其余为0的向量</p><h5 id="详见" tabindex="-1"><a class="header-anchor" href="#详见"><span>详见</span></a></h5>',57),f={href:"https://allenwind.github.io/blog/9905/",target:"_blank",rel:"noopener noreferrer"},x={href:"https://allenwind.github.io/blog/15110/",target:"_blank",rel:"noopener noreferrer"},_=t(`<h3 id="sampling" tabindex="-1"><a class="header-anchor" href="#sampling"><span>Sampling</span></a></h3><p>pre:</p><ul><li>softmax aim at:</li><li>temperature sample</li><li>nucleus sample (top-p)</li><li>top-k</li></ul><p>语言模型的输出(GPT-2)是通过自回归的方式输出的，将当前输出的token迭代进行下一次的预测，计算分布，得到概率输出。但是这样的输出经常出现问题(stuck or loop)</p><ul><li>如果只选最优概率，可能导致stuck and loop（注意力集中在后几个）</li><li>取样会更好</li><li>随机取可能会有很大一部分token和答案毫不相关，但是这部分token占有概率，被选中有可能导致推理脱轨</li><li>为了避免选中这些东西，流行的方法有temperature sampling and top k sampling</li></ul><h5 id="temperature-sampling" tabindex="-1"><a class="header-anchor" href="#temperature-sampling"><span>Temperature sampling</span></a></h5><p>inspired by statistical thermodynamics, where high temperature means low energy states are more likely encountered. 低能量的状态遇到的概率更大</p><ul><li>in probability models, logits(the raw output of the final linear layer)</li><li>before feeding to softmax, implement temperature sampling by dividing logits by the temperature.</li><li><strong>高温下，经过softmax，低概率的数的概率相对增大</strong></li></ul><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre class="language-python"><code><span class="line"><span class="token keyword">import</span> torch</span>
<span class="line"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F</span>
<span class="line">temperature <span class="token operator">=</span> <span class="token number">0.5</span>  <span class="token comment"># ex</span></span>
<span class="line">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span></span>
<span class="line">F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>a<span class="token operator">/</span>temperature<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>in a conclusion Lower temperatures make the model increasingly confident in its top choices, while temperatures greater than 1 decrease confidence.</p></li><li><p>温度越高，随机性越大</p></li><li><p>addition 0 temperature is equivalent to argmax/max likelihood, while infinite temperature corresponds to a uniform sampling.</p></li></ul><h6 id="为什么相对增大" tabindex="-1"><a class="header-anchor" href="#为什么相对增大"><span>为什么相对增大?</span></a></h6><p>softmax 定义式是： $$softmax(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^ne^{x_j}}$$ 加上$\\tau$ (temperature) $$softmax(x_i) = \\frac{e^{x_i/\\tau}}{\\sum_{j=1}^ne^{x_j/\\tau}}$$</p><ul><li>对于softmax的值来说，$x/\\tau$ ，x越大，对于$e^x$来说变化的幅度就越大(指数曲线)，所有$x_i$在x上的放大（缩小）倍数是一样的，原来概率大的token softmax出来之后，分子分母变化都大，而概率小的分母变化更大。</li><li>当温度低，概率小的token分母增大相对分子更明显，概率随温度低而减小</li><li>当温度高，概率小的token分母减小相对分子更明显，数值增大，而概率大的相反</li></ul><p>所以$\\tau$越大，低概率token抢占的概率更大，随机性越高。</p><h5 id="top-k-sampling" tabindex="-1"><a class="header-anchor" href="#top-k-sampling"><span>Top k sampling</span></a></h5><p><strong>sorting by probability and zeroing out the probabilities for anything below the k&#39;th token</strong> 除了前k个外都置为0</p><ul><li>it appears to improve the result</li><li>but in some cases, the probabilities of token can not be aparently seperated</li><li>可能多个token都有道理，概率分布的差异并不显著</li><li>propose nucleus(/ˈnjuː.kli.əs/) sampling</li></ul><h5 id="nucleus-sample" tabindex="-1"><a class="header-anchor" href="#nucleus-sample"><span>nucleus sample</span></a></h5><ul><li>top-p sampling, where p is the threshold of the CDF.</li><li>calculate the first n token and cut off as soon as CDF exceed p</li><li>保留了一定的随机性</li></ul><h5 id="why-maximum-likelihood-may-not-work" tabindex="-1"><a class="header-anchor" href="#why-maximum-likelihood-may-not-work"><span>Why maximum likelihood may not work?</span></a></h5><ul><li>it can never see the errors inside the model, it may predict a error token but use &quot;correct&quot; method for the next one, expanding the error.</li><li>或者，当只选取概率最大时，由于注意力只集中在后几个上，所以算出来可能会是单考虑特定token时的答案，此时会导向不合理的结果</li></ul><h5 id="取样之后呢" tabindex="-1"><a class="header-anchor" href="#取样之后呢"><span>取样之后呢</span></a></h5><ul><li>top-p top-k temperature 都是选出区间</li><li>temperature 和 top-k top-p可以一起使用</li><li>在这之后根据概率取样</li></ul><p>根据karpathy的手搓LLama2</p><ul><li>可以是设定一个0到1的随机数(coin)，将概率倒序排列</li><li>当分布函数CDF 取值超过 coin时，选出此时最后一个加入的token</li><li>coin的选定是随机的</li><li>coin落到哪里概率都一样，那coin是否落在一个token的概率范围里只和概率的大小有关，实现了概率取样，得到真正的next token</li></ul><div class="language-c line-numbers-mode" data-highlighter="prismjs" data-ext="c" data-title="c"><pre class="language-c"><code><span class="line"><span class="token keyword">int</span> <span class="token function">sample_mult</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token operator">*</span> probabilities<span class="token punctuation">,</span> <span class="token keyword">int</span> n<span class="token punctuation">,</span> <span class="token keyword">float</span> coin<span class="token punctuation">)</span> <span class="token punctuation">{</span></span>
<span class="line">    <span class="token comment">// sample index from probabilities (they must sum to 1!)</span></span>
<span class="line">    <span class="token comment">// coin is a random number in [0, 1), usually from random_f32()</span></span>
<span class="line">    <span class="token keyword">float</span> cdf <span class="token operator">=</span> <span class="token number">0.0f</span><span class="token punctuation">;</span></span>
<span class="line">    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span></span>
<span class="line">        cdf <span class="token operator">+=</span> probabilities<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span></span>
<span class="line">        <span class="token keyword">if</span> <span class="token punctuation">(</span>coin <span class="token operator">&lt;</span> cdf<span class="token punctuation">)</span> <span class="token punctuation">{</span></span>
<span class="line">            <span class="token keyword">return</span> i<span class="token punctuation">;</span></span>
<span class="line">        <span class="token punctuation">}</span></span>
<span class="line">    <span class="token punctuation">}</span></span>
<span class="line">    <span class="token keyword">return</span> n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token comment">// in case of rounding errors</span></span>
<span class="line"><span class="token punctuation">}</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>对于top-p来说，最后的计算只需限制前k个里面选，即在概率为cdf里面而不是在1里</p><div class="language-c line-numbers-mode" data-highlighter="prismjs" data-ext="c" data-title="c"><pre class="language-c"><code><span class="line"><span class="token keyword">int</span> <span class="token function">sample_topp</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token operator">*</span> probabilities<span class="token punctuation">,</span> <span class="token keyword">int</span> n<span class="token punctuation">,</span> <span class="token keyword">float</span> topp<span class="token punctuation">,</span> ProbIndex<span class="token operator">*</span> probindex<span class="token punctuation">,</span> <span class="token keyword">float</span> coin<span class="token punctuation">)</span> <span class="token punctuation">{</span></span>
<span class="line">    <span class="token comment">// top-p sampling (or &quot;nucleus sampling&quot;) samples from the smallest set of</span></span>
<span class="line">    <span class="token comment">// tokens that exceed probability topp. This way we never sample tokens that</span></span>
<span class="line">    <span class="token comment">// have very low probabilities and are less likely to go &quot;off the rails&quot;.</span></span>
<span class="line">    <span class="token comment">// coin is a random number in [0, 1), usually from random_f32()</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">int</span> n0 <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span></span>
<span class="line">    <span class="token comment">// quicksort indices in descending order of probabilities</span></span>
<span class="line">    <span class="token comment">// values smaller than (1 - topp) / (n - 1) cannot be part of the result</span></span>
<span class="line">    <span class="token comment">// so for efficiency we crop these out as candidates before sorting</span></span>
<span class="line">    <span class="token keyword">const</span> <span class="token keyword">float</span> cutoff <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1.0f</span> <span class="token operator">-</span> topp<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line">    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span></span>
<span class="line">        <span class="token keyword">if</span> <span class="token punctuation">(</span>probabilities<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&gt;=</span> cutoff<span class="token punctuation">)</span> <span class="token punctuation">{</span></span>
<span class="line">            probindex<span class="token punctuation">[</span>n0<span class="token punctuation">]</span><span class="token punctuation">.</span>index <span class="token operator">=</span> i<span class="token punctuation">;</span></span>
<span class="line">            probindex<span class="token punctuation">[</span>n0<span class="token punctuation">]</span><span class="token punctuation">.</span>prob <span class="token operator">=</span> probabilities<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span></span>
<span class="line">            n0<span class="token operator">++</span><span class="token punctuation">;</span></span>
<span class="line">        <span class="token punctuation">}</span></span>
<span class="line">    <span class="token punctuation">}</span></span>
<span class="line">    <span class="token function">qsort</span><span class="token punctuation">(</span>probindex<span class="token punctuation">,</span> n0<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>ProbIndex<span class="token punctuation">)</span><span class="token punctuation">,</span> compare<span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line"></span>
<span class="line">    <span class="token comment">// truncate the list where cumulative probability exceeds topp</span></span>
<span class="line">    <span class="token keyword">float</span> cumulative_prob <span class="token operator">=</span> <span class="token number">0.0f</span><span class="token punctuation">;</span></span>
<span class="line">    <span class="token keyword">int</span> last_idx <span class="token operator">=</span> n0 <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token comment">// in case of rounding errors consider all elements</span></span>
<span class="line">    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n0<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span></span>
<span class="line">        cumulative_prob <span class="token operator">+=</span> probindex<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>prob<span class="token punctuation">;</span></span>
<span class="line">        <span class="token keyword">if</span> <span class="token punctuation">(</span>cumulative_prob <span class="token operator">&gt;</span> topp<span class="token punctuation">)</span> <span class="token punctuation">{</span></span>
<span class="line">            last_idx <span class="token operator">=</span> i<span class="token punctuation">;</span></span>
<span class="line">            <span class="token keyword">break</span><span class="token punctuation">;</span> <span class="token comment">// we&#39;ve exceeded topp by including last_idx</span></span>
<span class="line">        <span class="token punctuation">}</span></span>
<span class="line">    <span class="token punctuation">}</span></span>
<span class="line"></span>
<span class="line">    <span class="token comment">// sample from the truncated list</span></span>
<span class="line">    <span class="token keyword">float</span> r <span class="token operator">=</span> coin <span class="token operator">*</span> cumulative_prob<span class="token punctuation">;</span></span>
<span class="line">    <span class="token keyword">float</span> cdf <span class="token operator">=</span> <span class="token number">0.0f</span><span class="token punctuation">;</span></span>
<span class="line">    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;=</span> last_idx<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span></span>
<span class="line">        cdf <span class="token operator">+=</span> probindex<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>prob<span class="token punctuation">;</span></span>
<span class="line">        <span class="token keyword">if</span> <span class="token punctuation">(</span>r <span class="token operator">&lt;</span> cdf<span class="token punctuation">)</span> <span class="token punctuation">{</span></span>
<span class="line">            <span class="token keyword">return</span> probindex<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>index<span class="token punctuation">;</span></span>
<span class="line">        <span class="token punctuation">}</span></span>
<span class="line">    <span class="token punctuation">}</span></span>
<span class="line">    <span class="token keyword">return</span> probindex<span class="token punctuation">[</span>last_idx<span class="token punctuation">]</span><span class="token punctuation">.</span>index<span class="token punctuation">;</span> <span class="token comment">// in case of rounding errors</span></span>
<span class="line"><span class="token punctuation">}</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="reference" tabindex="-1"><a class="header-anchor" href="#reference"><span>Reference</span></a></h2>`,29),$={href:"https://www.youtube.com/watch?v=n9TlOhRjYoc&t=60s",target:"_blank",rel:"noopener noreferrer"},y={href:"https://www.youtube.com/watch?v=N6aRv06iv2g&t=1574s",target:"_blank",rel:"noopener noreferrer"},w={href:"https://www.youtube.com/watch?v=nzqlFIcCSWQ",target:"_blank",rel:"noopener noreferrer"};function P(q,E){const s=l("ExternalLinkIcon");return p(),o("div",null,[r,n("p",null,[u,n("a",d,[a("Moonshot AI - 开放平台"),e(s)])]),m,n("ul",null,[k,h,n("li",null,[n("p",null,[a("self attention之前就有人提出类似的机制，但是不叫这个名字，在"),n("a",b,[a("[1706.03762] Attention Is All You Need"),e(s)]),a(" 中正式提出，同时提出了Transformer")])]),g]),v,n("ul",null,[n("li",null,[n("a",f,[a("函数光滑近似（2）：softmax与argmax | Erwin Feng Blog"),e(s)])]),n("li",null,[n("a",x,[a("分析与拓展：多分类模型的输出为什么使用softmax？ | Erwin Feng Blog"),e(s)])])]),_,n("ul",null,[n("li",null,[n("a",$,[a("李宏毅【機器學習2021】Transformer (上) - YouTube"),e(s)])]),n("li",null,[n("a",y,[a("李宏毅【機器學習2021】Transformer (下) - YouTube"),e(s)])]),n("li",null,[n("a",w,[a("李沐Transformer论文逐段精读 - YouTube"),e(s)])])])])}const j=i(c,[["render",P],["__file","damoxingdiercilihui.html.vue"]]),I=JSON.parse('{"path":"/docs/ai/damoxingdiercilihui.html","title":"大模型第二次例会","lang":"zh-CN","frontmatter":{"title":"大模型第二次例会","author":"saber","date":"2024/9/9"},"headers":[{"level":2,"title":"前置条件","slug":"前置条件","link":"#前置条件","children":[]},{"level":2,"title":"大概纲要","slug":"大概纲要","link":"#大概纲要","children":[]},{"level":2,"title":"Transformer","slug":"transformer","link":"#transformer","children":[]},{"level":2,"title":"API 参数","slug":"api-参数","link":"#api-参数","children":[{"level":3,"title":"关于softmax","slug":"关于softmax","link":"#关于softmax","children":[]},{"level":3,"title":"Sampling","slug":"sampling","link":"#sampling","children":[]}]},{"level":2,"title":"Reference","slug":"reference","link":"#reference","children":[]}],"git":{"createdTime":1725811061000,"updatedTime":1725811061000,"contributors":[{"name":"saber","email":"wuyacwc@gmail.com","commits":1}]},"filePathRelative":"docs/ai/大模型第二次例会.md"}');export{j as comp,I as data};
